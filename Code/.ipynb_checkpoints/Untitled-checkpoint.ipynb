{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7e4f5d",
   "metadata": {},
   "source": [
    "# Sourcing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370f5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db51321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasdaqdatalink\n",
    "import os\n",
    "import pandas as pd\n",
    "nasdaqdatalink.ApiConfig.api_key = \"6X-12CyLywuaJiyhmnzi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24214328",
   "metadata": {},
   "source": [
    "## What data do we want? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf35656",
   "metadata": {},
   "source": [
    "- Futures prices for a single commodity\n",
    "- We want to get rolling price data, but the API will only give us data for a fixed month in a fixed year.\n",
    "- Okay let's just try and get some data for say all the contracts in 2021.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2eb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "data_link_log = logging.getLogger('nasdaqdatalink')\n",
    "data_link_log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b196d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['H', 'K', 'N', 'U', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e594bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ubaidhoque/Projects/Dissertation/Code/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3340: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/4172720896.py:13: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Settle_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame()\n",
    "dataset = dataset.reindex(pd.date_range(start='1/1/2007', end='26/02/2022', freq='B'))\n",
    "dataset = dataset.set_index(dataset.index.rename(\"Date\"))\n",
    "months_dict = dict({'H': \"March\", 'K': \"May\", 'N': \"July\", 'U': \"September\", 'Z': \"December\"})\n",
    "column_names = []\n",
    "\n",
    "\n",
    "for year in range(2010, 2025):\n",
    "    for cct in months:\n",
    "        label = 'SRF/CME_C{}'.format(cct + str(year))\n",
    "        row = pd.DataFrame(nasdaqdatalink.get(label)[\"Settle\"])\n",
    "        column_names.append(cct+str(year))\n",
    "        dataset = dataset.merge(row, on=\"Date\", how=\"left\")\n",
    "        \n",
    "dataset.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59b3c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H2010</th>\n",
       "      <th>K2010</th>\n",
       "      <th>N2010</th>\n",
       "      <th>U2010</th>\n",
       "      <th>Z2010</th>\n",
       "      <th>H2011</th>\n",
       "      <th>K2011</th>\n",
       "      <th>N2011</th>\n",
       "      <th>U2011</th>\n",
       "      <th>Z2011</th>\n",
       "      <th>...</th>\n",
       "      <th>H2023</th>\n",
       "      <th>K2023</th>\n",
       "      <th>N2023</th>\n",
       "      <th>U2023</th>\n",
       "      <th>Z2023</th>\n",
       "      <th>H2024</th>\n",
       "      <th>K2024</th>\n",
       "      <th>N2024</th>\n",
       "      <th>U2024</th>\n",
       "      <th>Z2024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>604.75</td>\n",
       "      <td>608.25</td>\n",
       "      <td>609.75</td>\n",
       "      <td>569.00</td>\n",
       "      <td>560.25</td>\n",
       "      <td>567.25</td>\n",
       "      <td>566.75</td>\n",
       "      <td>568.25</td>\n",
       "      <td>518.50</td>\n",
       "      <td>509.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>612.75</td>\n",
       "      <td>616.25</td>\n",
       "      <td>617.75</td>\n",
       "      <td>576.75</td>\n",
       "      <td>567.75</td>\n",
       "      <td>574.50</td>\n",
       "      <td>574.00</td>\n",
       "      <td>575.75</td>\n",
       "      <td>524.75</td>\n",
       "      <td>512.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>618.00</td>\n",
       "      <td>621.50</td>\n",
       "      <td>622.75</td>\n",
       "      <td>579.25</td>\n",
       "      <td>570.00</td>\n",
       "      <td>576.50</td>\n",
       "      <td>576.00</td>\n",
       "      <td>578.00</td>\n",
       "      <td>527.00</td>\n",
       "      <td>514.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611.00</td>\n",
       "      <td>614.25</td>\n",
       "      <td>615.50</td>\n",
       "      <td>571.75</td>\n",
       "      <td>563.50</td>\n",
       "      <td>570.50</td>\n",
       "      <td>570.00</td>\n",
       "      <td>571.75</td>\n",
       "      <td>520.75</td>\n",
       "      <td>502.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>586.75</td>\n",
       "      <td>590.00</td>\n",
       "      <td>591.50</td>\n",
       "      <td>551.00</td>\n",
       "      <td>543.00</td>\n",
       "      <td>550.00</td>\n",
       "      <td>549.50</td>\n",
       "      <td>551.25</td>\n",
       "      <td>500.25</td>\n",
       "      <td>489.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3955 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            H2010  K2010  N2010  U2010  Z2010  H2011  K2011  N2011  U2011  \\\n",
       "Date                                                                        \n",
       "2007-01-01    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2007-01-02    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2007-01-03    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2007-01-04    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2007-01-05    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2022-02-21    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2022-02-22    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2022-02-23    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2022-02-24    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2022-02-25    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "            Z2011  ...   H2023   K2023   N2023   U2023   Z2023   H2024  \\\n",
       "Date               ...                                                   \n",
       "2007-01-01    NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2007-01-02    NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2007-01-03    NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2007-01-04    NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2007-01-05    NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...           ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "2022-02-21    NaN  ...  604.75  608.25  609.75  569.00  560.25  567.25   \n",
       "2022-02-22    NaN  ...  612.75  616.25  617.75  576.75  567.75  574.50   \n",
       "2022-02-23    NaN  ...  618.00  621.50  622.75  579.25  570.00  576.50   \n",
       "2022-02-24    NaN  ...  611.00  614.25  615.50  571.75  563.50  570.50   \n",
       "2022-02-25    NaN  ...  586.75  590.00  591.50  551.00  543.00  550.00   \n",
       "\n",
       "             K2024   N2024   U2024   Z2024  \n",
       "Date                                        \n",
       "2007-01-01     NaN     NaN     NaN     NaN  \n",
       "2007-01-02     NaN     NaN     NaN     NaN  \n",
       "2007-01-03     NaN     NaN     NaN     NaN  \n",
       "2007-01-04     NaN     NaN     NaN     NaN  \n",
       "2007-01-05     NaN     NaN     NaN     NaN  \n",
       "...            ...     ...     ...     ...  \n",
       "2022-02-21  566.75  568.25  518.50  509.00  \n",
       "2022-02-22  574.00  575.75  524.75  512.50  \n",
       "2022-02-23  576.00  578.00  527.00  514.00  \n",
       "2022-02-24  570.00  571.75  520.75  502.50  \n",
       "2022-02-25  549.50  551.25  500.25  489.75  \n",
       "\n",
       "[3955 rows x 75 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66190278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(month):\n",
    "    if month in {1,2,3}:\n",
    "        return 0\n",
    "    elif month in {4,5}:\n",
    "        return 1\n",
    "    elif month in {6,7}:\n",
    "        return 2\n",
    "    elif month in {8,9}:\n",
    "        return 3\n",
    "    elif month in {10,11,0}:\n",
    "        return 4\n",
    "\n",
    "def get_relative_future(contracts_ahead, fixed_dataset, current_date):\n",
    "    day = current_date.day\n",
    "    month = current_date.month\n",
    "    if day >= 14 and month in {3,5,7,9,12}:\n",
    "        return roll((month+1)%12) + contracts_ahead\n",
    "    else:\n",
    "        return roll(month%12) + contracts_ahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8d3f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ubaidhoque/Projects/Dissertation/Code/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3340: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "rolling_dataset = pd.DataFrame()\n",
    "rolling_dataset = rolling_dataset.reindex(pd.date_range(start='1/1/2010', end='26/02/2022', freq='B'))\n",
    "rolling_dataset = rolling_dataset.set_index(rolling_dataset.index.rename(\"Date\"))\n",
    "\n",
    "rolling_prices = []\n",
    "for i, date in enumerate(pd.date_range(start='1/1/2010', end='26/02/2022', freq='B')):\n",
    "    cct = []\n",
    "    for contracts_ahead in range(0, 10):\n",
    "        years_added, contract_index = divmod(get_relative_future(contracts_ahead, dataset, date), 5)\n",
    "        price = dataset[months[contract_index] + str((date + relativedelta(years=years_added)).year)][date]\n",
    "        #price = dataset[get_relative_future(contracts_ahead, dataset, date) + str(date.year)][date]\n",
    "        cct.append(price)\n",
    "        #print(get_relative_future(contracts_ahead, dataset, date))\n",
    "        #print(price)\n",
    "    rolling_prices.append(cct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff09d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_dataset = pd.DataFrame(rolling_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f910cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>418.50</td>\n",
       "      <td>428.75</td>\n",
       "      <td>437.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>445.25</td>\n",
       "      <td>454.00</td>\n",
       "      <td>460.25</td>\n",
       "      <td>465.25</td>\n",
       "      <td>456.25</td>\n",
       "      <td>451.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>418.75</td>\n",
       "      <td>429.00</td>\n",
       "      <td>437.50</td>\n",
       "      <td>441.75</td>\n",
       "      <td>445.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421.75</td>\n",
       "      <td>432.00</td>\n",
       "      <td>440.75</td>\n",
       "      <td>444.75</td>\n",
       "      <td>448.75</td>\n",
       "      <td>458.25</td>\n",
       "      <td>464.25</td>\n",
       "      <td>469.50</td>\n",
       "      <td>460.75</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417.50</td>\n",
       "      <td>428.00</td>\n",
       "      <td>436.75</td>\n",
       "      <td>440.00</td>\n",
       "      <td>443.50</td>\n",
       "      <td>453.00</td>\n",
       "      <td>459.00</td>\n",
       "      <td>464.25</td>\n",
       "      <td>455.50</td>\n",
       "      <td>452.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>654.25</td>\n",
       "      <td>652.75</td>\n",
       "      <td>647.00</td>\n",
       "      <td>608.75</td>\n",
       "      <td>597.75</td>\n",
       "      <td>604.75</td>\n",
       "      <td>608.25</td>\n",
       "      <td>609.75</td>\n",
       "      <td>569.00</td>\n",
       "      <td>560.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>674.75</td>\n",
       "      <td>672.50</td>\n",
       "      <td>665.25</td>\n",
       "      <td>621.00</td>\n",
       "      <td>605.75</td>\n",
       "      <td>612.75</td>\n",
       "      <td>616.25</td>\n",
       "      <td>617.75</td>\n",
       "      <td>576.75</td>\n",
       "      <td>567.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>683.75</td>\n",
       "      <td>681.25</td>\n",
       "      <td>674.50</td>\n",
       "      <td>628.75</td>\n",
       "      <td>611.25</td>\n",
       "      <td>618.00</td>\n",
       "      <td>621.50</td>\n",
       "      <td>622.75</td>\n",
       "      <td>579.25</td>\n",
       "      <td>570.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>695.00</td>\n",
       "      <td>690.25</td>\n",
       "      <td>678.50</td>\n",
       "      <td>620.75</td>\n",
       "      <td>604.75</td>\n",
       "      <td>611.00</td>\n",
       "      <td>614.25</td>\n",
       "      <td>615.50</td>\n",
       "      <td>571.75</td>\n",
       "      <td>563.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>659.50</td>\n",
       "      <td>655.75</td>\n",
       "      <td>644.00</td>\n",
       "      <td>594.25</td>\n",
       "      <td>579.75</td>\n",
       "      <td>586.75</td>\n",
       "      <td>590.00</td>\n",
       "      <td>591.50</td>\n",
       "      <td>551.00</td>\n",
       "      <td>543.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3171 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1       2       3       4       5       6       7       8  \\\n",
       "0        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "1     418.50  428.75  437.00  441.00  445.25  454.00  460.25  465.25  456.25   \n",
       "2     418.75  429.00  437.50  441.75  445.25     NaN     NaN     NaN     NaN   \n",
       "3     421.75  432.00  440.75  444.75  448.75  458.25  464.25  469.50  460.75   \n",
       "4     417.50  428.00  436.75  440.00  443.50  453.00  459.00  464.25  455.50   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "3166  654.25  652.75  647.00  608.75  597.75  604.75  608.25  609.75  569.00   \n",
       "3167  674.75  672.50  665.25  621.00  605.75  612.75  616.25  617.75  576.75   \n",
       "3168  683.75  681.25  674.50  628.75  611.25  618.00  621.50  622.75  579.25   \n",
       "3169  695.00  690.25  678.50  620.75  604.75  611.00  614.25  615.50  571.75   \n",
       "3170  659.50  655.75  644.00  594.25  579.75  586.75  590.00  591.50  551.00   \n",
       "\n",
       "           9  \n",
       "0        NaN  \n",
       "1     451.25  \n",
       "2        NaN  \n",
       "3     457.75  \n",
       "4     452.50  \n",
       "...      ...  \n",
       "3166  560.25  \n",
       "3167  567.75  \n",
       "3168  570.00  \n",
       "3169  563.50  \n",
       "3170  543.00  \n",
       "\n",
       "[3171 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c506adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ubaidhoque/Projects/Dissertation/Code/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3340: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "rolling_dataset = rolling_dataset.set_index(pd.date_range(start='1/1/2010', end='26/02/2022', freq='B'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8c4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_rolling_dataset = rolling_dataset.interpolate().pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdae828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_rolling_dataset[2].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f98e94",
   "metadata": {},
   "source": [
    "# How can we generate time series data that would look like the commodity futures we have at the moment? \n",
    "\n",
    "The CBOT Corn futures are available for March (3)(H), May (5)(K), July (7)(N), Sep(9)(U), Dec (12)(Z) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53e2a2",
   "metadata": {},
   "source": [
    "## Steps \n",
    "\n",
    "1. Construct a rolling dataset\n",
    "    - It is possible to obtain a dataset or matrix with the rows as the dates and then the columns as the contracts. \n",
    "    - What we want to do is if say we want to look at 8 contracts into the future, then we want our dataset to have more than 8 columns, and then we want to use the date index to swap the columns when needed to get this rolling behaviour.\n",
    "    - It is probably worth at this point to look at the different rolling mechanisms that are available.  May be more difficult to implement on the Macquarie side but at least we know that we've done the research part of it.  I think this is what we should do on the train back today.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08322a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingFutures(): \n",
    "    def init(self, start, end, asset, database, contracts):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.asset = asset\n",
    "        self.database = database\n",
    "        self.contracts = contracts\n",
    "\n",
    "    def roll(month):\n",
    "        if month in {1,2,3}:\n",
    "            return 0\n",
    "        elif month in {4,5}:\n",
    "            return 1\n",
    "        elif month in {6,7}:\n",
    "            return 2\n",
    "        elif month in {8,9}:\n",
    "            return 3\n",
    "        elif month in {10,11,12, 0}:\n",
    "            return 4\n",
    "\n",
    "    def get_relative_future(contracts_ahead, fixed_dataset, current_date):\n",
    "        day = current_date.day\n",
    "        month = current_date.month\n",
    "        if day >= 14 and month in {3,5,7,9,12}:\n",
    "            return months[(roll((month+1)%12) + contracts_ahead)%5]\n",
    "        else:\n",
    "            return months[(roll(month%12) + contracts_ahead)%5]\n",
    "\n",
    "    def print_adf(data):\n",
    "\n",
    "        from pandas import read_csv\n",
    "        from statsmodels.tsa.stattools import adfuller\n",
    "        X = data.dropna().values\n",
    "        result = adfuller(X)\n",
    "        print('ADF Statistic: %f' % result[0])\n",
    "        print('p-value: %f' % result[1])\n",
    "        print('Critical Values:')\n",
    "        for key, value in result[4].items():\n",
    "            print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df6fbc",
   "metadata": {},
   "source": [
    "## We finally have data of an appropriate form to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09aeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first check for stationarity.\n",
    "\n",
    "def calc_adf(data):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    X = data.dropna().values\n",
    "    result = adfuller(X)\n",
    "    return result\n",
    "\n",
    "def print_adf(result):\n",
    "\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58471cc5",
   "metadata": {},
   "source": [
    "## Creating derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab367288",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_rolling_dataset = rolling_dataset.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7b50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to define some derived variables from the data that we have to feed into our HMM.\n",
    "\n",
    "# LEVEL \n",
    "level = interpolated_rolling_dataset.mean(axis=1).dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "858e13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility \n",
    "\n",
    "# We want the moving average over an x-day window\n",
    "\n",
    "def mse_volatility(data):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    mean = data.mean()\n",
    "    means = np.full((len(data)), mean)\n",
    "    return mse(data, means)\n",
    "\n",
    "volatility = level.rolling(10, min_periods=10).apply(mse_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4140f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope \n",
    "import numpy as np\n",
    "slope = interpolated_rolling_dataset[9] - interpolated_rolling_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0280dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the derived variables as columns in our dataframe\n",
    "\n",
    "interpolated_rolling_dataset['level'] = level\n",
    "interpolated_rolling_dataset['volatility'] = volatility\n",
    "interpolated_rolling_dataset['slope'] = slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f98540",
   "metadata": {},
   "source": [
    "## Categorise Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10d8a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ('level', 'volatility'):\n",
    "    data = interpolated_rolling_dataset[var]\n",
    "    bins = [np.nanquantile(data, x) for x in [0, .2, .4, .6, .8, 1]]\n",
    "    group_names = [1, 2, 3, 4, 5]\n",
    "    interpolated_rolling_dataset['cat_' + var] = pd.cut(data, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15fa24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_rolling_dataset['cat_slope'] = pd.cut(interpolated_rolling_dataset['slope'], [-500, 0, 500], labels=['Contango', 'Backwardation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "894e2a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>level</th>\n",
       "      <th>volatility</th>\n",
       "      <th>slope</th>\n",
       "      <th>cat_level</th>\n",
       "      <th>cat_volatility</th>\n",
       "      <th>cat_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>418.50</td>\n",
       "      <td>428.75</td>\n",
       "      <td>437.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>445.25</td>\n",
       "      <td>454.000</td>\n",
       "      <td>460.25</td>\n",
       "      <td>465.250</td>\n",
       "      <td>456.25</td>\n",
       "      <td>451.25</td>\n",
       "      <td>445.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Backwardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>418.75</td>\n",
       "      <td>429.00</td>\n",
       "      <td>437.50</td>\n",
       "      <td>441.75</td>\n",
       "      <td>445.25</td>\n",
       "      <td>456.125</td>\n",
       "      <td>462.25</td>\n",
       "      <td>467.375</td>\n",
       "      <td>458.50</td>\n",
       "      <td>454.50</td>\n",
       "      <td>447.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Backwardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>421.75</td>\n",
       "      <td>432.00</td>\n",
       "      <td>440.75</td>\n",
       "      <td>444.75</td>\n",
       "      <td>448.75</td>\n",
       "      <td>458.250</td>\n",
       "      <td>464.25</td>\n",
       "      <td>469.500</td>\n",
       "      <td>460.75</td>\n",
       "      <td>457.75</td>\n",
       "      <td>449.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Backwardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>417.50</td>\n",
       "      <td>428.00</td>\n",
       "      <td>436.75</td>\n",
       "      <td>440.00</td>\n",
       "      <td>443.50</td>\n",
       "      <td>453.000</td>\n",
       "      <td>459.00</td>\n",
       "      <td>464.250</td>\n",
       "      <td>455.50</td>\n",
       "      <td>452.50</td>\n",
       "      <td>445.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Backwardation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-21</th>\n",
       "      <td>654.25</td>\n",
       "      <td>652.75</td>\n",
       "      <td>647.00</td>\n",
       "      <td>608.75</td>\n",
       "      <td>597.75</td>\n",
       "      <td>604.750</td>\n",
       "      <td>608.25</td>\n",
       "      <td>609.750</td>\n",
       "      <td>569.00</td>\n",
       "      <td>560.25</td>\n",
       "      <td>611.250</td>\n",
       "      <td>29.947025</td>\n",
       "      <td>-94.00</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Contango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>674.75</td>\n",
       "      <td>672.50</td>\n",
       "      <td>665.25</td>\n",
       "      <td>621.00</td>\n",
       "      <td>605.75</td>\n",
       "      <td>612.750</td>\n",
       "      <td>616.25</td>\n",
       "      <td>617.750</td>\n",
       "      <td>576.75</td>\n",
       "      <td>567.75</td>\n",
       "      <td>623.050</td>\n",
       "      <td>40.070500</td>\n",
       "      <td>-107.00</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Contango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>683.75</td>\n",
       "      <td>681.25</td>\n",
       "      <td>674.50</td>\n",
       "      <td>628.75</td>\n",
       "      <td>611.25</td>\n",
       "      <td>618.000</td>\n",
       "      <td>621.50</td>\n",
       "      <td>622.750</td>\n",
       "      <td>579.25</td>\n",
       "      <td>570.00</td>\n",
       "      <td>629.100</td>\n",
       "      <td>72.884600</td>\n",
       "      <td>-113.75</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Contango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>695.00</td>\n",
       "      <td>690.25</td>\n",
       "      <td>678.50</td>\n",
       "      <td>620.75</td>\n",
       "      <td>604.75</td>\n",
       "      <td>611.000</td>\n",
       "      <td>614.25</td>\n",
       "      <td>615.500</td>\n",
       "      <td>571.75</td>\n",
       "      <td>563.50</td>\n",
       "      <td>626.525</td>\n",
       "      <td>74.925756</td>\n",
       "      <td>-131.50</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Contango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>659.50</td>\n",
       "      <td>655.75</td>\n",
       "      <td>644.00</td>\n",
       "      <td>594.25</td>\n",
       "      <td>579.75</td>\n",
       "      <td>586.750</td>\n",
       "      <td>590.00</td>\n",
       "      <td>591.500</td>\n",
       "      <td>551.00</td>\n",
       "      <td>543.00</td>\n",
       "      <td>599.550</td>\n",
       "      <td>91.569850</td>\n",
       "      <td>-116.50</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Contango</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3171 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1       2       3       4        5       6        7  \\\n",
       "2010-01-01     NaN     NaN     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "2010-01-04  418.50  428.75  437.00  441.00  445.25  454.000  460.25  465.250   \n",
       "2010-01-05  418.75  429.00  437.50  441.75  445.25  456.125  462.25  467.375   \n",
       "2010-01-06  421.75  432.00  440.75  444.75  448.75  458.250  464.25  469.500   \n",
       "2010-01-07  417.50  428.00  436.75  440.00  443.50  453.000  459.00  464.250   \n",
       "...            ...     ...     ...     ...     ...      ...     ...      ...   \n",
       "2022-02-21  654.25  652.75  647.00  608.75  597.75  604.750  608.25  609.750   \n",
       "2022-02-22  674.75  672.50  665.25  621.00  605.75  612.750  616.25  617.750   \n",
       "2022-02-23  683.75  681.25  674.50  628.75  611.25  618.000  621.50  622.750   \n",
       "2022-02-24  695.00  690.25  678.50  620.75  604.75  611.000  614.25  615.500   \n",
       "2022-02-25  659.50  655.75  644.00  594.25  579.75  586.750  590.00  591.500   \n",
       "\n",
       "                 8       9    level  volatility   slope cat_level  \\\n",
       "2010-01-01     NaN     NaN      NaN         NaN     NaN       NaN   \n",
       "2010-01-04  456.25  451.25  445.750         NaN   32.75         3   \n",
       "2010-01-05  458.50  454.50  447.100         NaN   35.75         3   \n",
       "2010-01-06  460.75  457.75  449.850         NaN   36.00         3   \n",
       "2010-01-07  455.50  452.50  445.000         NaN   35.00         3   \n",
       "...            ...     ...      ...         ...     ...       ...   \n",
       "2022-02-21  569.00  560.25  611.250   29.947025  -94.00         5   \n",
       "2022-02-22  576.75  567.75  623.050   40.070500 -107.00         5   \n",
       "2022-02-23  579.25  570.00  629.100   72.884600 -113.75         5   \n",
       "2022-02-24  571.75  563.50  626.525   74.925756 -131.50         5   \n",
       "2022-02-25  551.00  543.00  599.550   91.569850 -116.50         5   \n",
       "\n",
       "           cat_volatility      cat_slope  \n",
       "2010-01-01            NaN            NaN  \n",
       "2010-01-04            NaN  Backwardation  \n",
       "2010-01-05            NaN  Backwardation  \n",
       "2010-01-06            NaN  Backwardation  \n",
       "2010-01-07            NaN  Backwardation  \n",
       "...                   ...            ...  \n",
       "2022-02-21              3       Contango  \n",
       "2022-02-22              4       Contango  \n",
       "2022-02-23              4       Contango  \n",
       "2022-02-24              4       Contango  \n",
       "2022-02-25              5       Contango  \n",
       "\n",
       "[3171 rows x 16 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_rolling_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46058e09",
   "metadata": {},
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86dc7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608c8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec74c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianHMM(n_components=2, covariance_type='full', n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be434fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianHMM(covariance_type='full', n_components=2, n_iter=100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_level = np.array(interpolated_rolling_dataset['slope'].dropna()).reshape(-1, 1) \n",
    "model.fit(reshaped_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "925cf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = model.predict(reshaped_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0af3ae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99743781, 0.00256219],\n",
       "       [0.00328865, 0.99671135]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bfd7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_rolling_dataset.index.name = 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16a5b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_rolling_dataset['hidden_states'] = np.concatenate((np.repeat(hidden_states[0],1), hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b28ddfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_univariate(): \n",
    "    def __init__(self, df: dict, start_date, end_date):\n",
    "        self.df = df\n",
    "        self.len = len(df)\n",
    "        self.contracts = len(df.columns)\n",
    "        self.date_idx = pd.date_range(start=self.start_date, end=self.end_date, freq='B')\n",
    "        \n",
    "    def perform_pca(self):\n",
    "        return\n",
    "    \n",
    "    def calc_level(self): \n",
    "        level = df.mean(axis=1).dropna()\n",
    "        self.df.level = level\n",
    "        \n",
    "    def calc_mse_error(self):\n",
    "        import numpy as np\n",
    "        from sklearn.metrics import mean_squared_error as mse\n",
    "        mean = self.df.mean()\n",
    "        means = np.full((len(self.df)), mean)\n",
    "        return mse(data, means)\n",
    "        \n",
    "    def calc_mse_volatility(self, window):\n",
    "        self.window = window\n",
    "        self.volatility = level.rolling(window, min_periods=window).apply(self.calc_mse_error)\n",
    "\n",
    "    def calc_slope(self, front: int, back: int):\n",
    "        self.slope = self.df[back] - self.df[front]\n",
    "        \n",
    "    def calc_hidden_states(self, no_h_states, ts):\n",
    "        from hmmlearn.hmm import GaussianHMM\n",
    "        self.model = GaussianHMM(no_h_states, covariance_type='full', n_iter=100)\n",
    "        self.model.fit(self.df[ts])\n",
    "        self.hidden_states = self.model.predict(self.df[ts])\n",
    "    \n",
    "    def regime_plot(self, ts):\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        # Create a new dataframe with a column for each state and and entries NA when not in that state on given day\n",
    "        \n",
    "        self.unique_states = self.hidden_states.nunique()\n",
    "        hs_df = pd.DataFrame()\n",
    "        hs_df.reindex(self.date_idx, inplace='True')\n",
    "        hs_df.set_index(hs_df.index.rename(\"Date\"), inplace='True')\n",
    "        for state in unique_states:\n",
    "            hs_data = self.df[self.df.hidden_state == state] \n",
    "            hs_df.merge(hs_data[ts], how='left', on='Date')\n",
    "        \n",
    "        for col in hs_df.columns:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.date_idx, hd_df[col])\n",
    "\n",
    "        ax.set_xaxis('Date')\n",
    "        ax.set_yaxis('Closing Price')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f7784dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_rolling_dataset['hidden_states'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c11adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interpolated_rolling_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ba77cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_state = interpolated_rolling_dataset[interpolated_rolling_dataset['hidden_states'] == 0]\n",
    "one_state = interpolated_rolling_dataset[interpolated_rolling_dataset['hidden_states'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d11867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-5.51886870e-03  4.79550840e-01]\n",
      "  [-3.60553796e-01  7.04706568e-01]]\n",
      "\n",
      " [[-3.60553796e-01  7.04706568e-01]\n",
      "  [-6.69363095e-01  5.42969673e-01]]\n",
      "\n",
      " [[-6.69363095e-01  5.42969673e-01]\n",
      "  [-8.60664351e-01  9.90005661e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.02088342e+01  1.13659556e+00]\n",
      "  [ 1.03569393e+01  1.51171361e+00]]\n",
      "\n",
      " [[ 1.03569393e+01  1.51171361e+00]\n",
      "  [ 1.06474598e+01  1.15282120e+00]]\n",
      "\n",
      " [[ 1.06474598e+01  1.15282120e+00]\n",
      "  [ 1.06930419e+01  7.58412749e-01]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "xy = (np.random.random((1000, 2)) - 0.5).cumsum(axis=0)\n",
    "# Reshape things so that we have a sequence of:\n",
    "# [[(x0,y0),(x1,y1)],[(x0,y0),(x1,y1)],...]\n",
    "xy = xy.reshape(-1, 1, 2)\n",
    "\n",
    "segments = np.hstack([xy[:-1], xy[1:]])\n",
    "print(segments)\n",
    "fig, ax = plt.subplots()\n",
    "coll = LineCollection(segments, cmap=plt.cm.gist_ncar)\n",
    "coll.set_array(df['hidden_states'])\n",
    "\n",
    "ax.add_collection(coll)\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e2ba5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame()\n",
    "plot_df = plot_df.reindex(pd.date_range(start='1/1/2007', end='26/02/2022', freq='B'))\n",
    "plot_df = plot_df.set_index(plot_df.index.rename(\"Date\"))\n",
    "plot_df = plot_df.merge(zero_state['level'], how='left', on='Date')\n",
    "plot_df = plot_df.merge(one_state['level'], how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10a41bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_x</th>\n",
       "      <th>level_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>611.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>623.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>629.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>626.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>599.550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3955 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            level_x  level_y\n",
       "Date                        \n",
       "2007-01-01      NaN      NaN\n",
       "2007-01-02      NaN      NaN\n",
       "2007-01-03      NaN      NaN\n",
       "2007-01-04      NaN      NaN\n",
       "2007-01-05      NaN      NaN\n",
       "...             ...      ...\n",
       "2022-02-21      NaN  611.250\n",
       "2022-02-22      NaN  623.050\n",
       "2022-02-23      NaN  629.100\n",
       "2022-02-24      NaN  626.525\n",
       "2022-02-25      NaN  599.550\n",
       "\n",
       "[3955 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f51f2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ubaidhoque/Projects/Dissertation/Code/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3340: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/ubaidhoque/Projects/Dissertation/Code/env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3340: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(pd.date_range(start='1/1/2007', end='26/02/2022', freq='B'), plot_df['level_x'])\n",
    "ax.plot(pd.date_range(start='1/1/2007', end='26/02/2022', freq='B'), plot_df['level_y'])\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d3d8e",
   "metadata": {},
   "source": [
    "## Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\n",
    "\n",
    "Note that we still need to figure out which rolling mechanism Macquarie use within the DataAG class. \n",
    "\n",
    "We want to use the AIC and BIC score to identify the optimal number of hidden states.  Of course, more states will lead to a higher score, but you want to add a penalty for that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf32747",
   "metadata": {},
   "source": [
    "## Remaining Steps\n",
    "\n",
    "1. Figure out how to use AIC and BIC to select optimal number of hidden states.\n",
    "2. Apply PCA and identify the regimes.\n",
    "3. Check for stationarity in the first principle component. \n",
    "4. Use the regimes to then obatin subset principle components. \n",
    "5. Go back and split into training and test data.\n",
    "6. Find the residual on most recent training day.\n",
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df3be6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interpolated_rolling_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cc73ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab2f58e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "X = df.iloc[:, :10].dropna()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c36ab82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16da201f0>]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c8e32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(pca.transform(X))\n",
    "pc1 = scores.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a35d81fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 298., 1403.,  241.,  214.,  192.,  260.,  231.,  128.,  144.,\n",
       "          59.]),\n",
       " array([-4.03087863, -2.76542216, -1.49996568, -0.23450921,  1.03094726,\n",
       "         2.29640374,  3.56186021,  4.82731668,  6.09277316,  7.35822963,\n",
       "         8.6236861 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(scores.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29358b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62449153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.47025511076910353, pvalue=0.0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kstest(scores.iloc[:, 0], 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5bc5997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.09400631, 0.53659306, 0.6126183 , 0.68012618, 0.74069401,\n",
       "        0.82271293, 0.8955836 , 0.93596215, 0.98138801, 1.        ]),\n",
       " array([-4.03087863, -2.76542216, -1.49996568, -0.23450921,  1.03094726,\n",
       "         2.29640374,  3.56186021,  4.82731668,  6.09277316,  7.35822963,\n",
       "         8.6236861 ]),\n",
       " [<matplotlib.patches.Polygon at 0x15bb62610>])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist(scores.iloc[:, 0], density=True, cumulative=True, label='CDF',\n",
    "         histtype='step', alpha=0.8, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce7cc8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15bb39430>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing random values\n",
    "N=500\n",
    "mu = pc1.mean()\n",
    "sigma = pc1.std()\n",
    "data = np.random.normal(mu, sigma, N)\n",
    "  \n",
    "# getting data of the histogram\n",
    "count, bins_count = np.histogram(data, bins=10)\n",
    "  \n",
    "# finding the PDF of the histogram using count values\n",
    "pdf = count / sum(count)\n",
    "  \n",
    "# using numpy np.cumsum to calculate the CDF\n",
    "# We can also find using the PDF values by looping and adding\n",
    "cdf = np.cumsum(pdf)\n",
    "  \n",
    "# plotting PDF and CDF\n",
    "plt.plot(bins_count[1:], cdf, label=\"CDF\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a2c4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1       -8630.2220             +nan\n",
      "         2       -7044.4577       +1585.7643\n",
      "         3       -5545.3195       +1499.1382\n",
      "         4       -4743.1158        +802.2037\n",
      "         5       -3991.9348        +751.1810\n",
      "         6       -3872.7114        +119.2234\n",
      "         7       -3865.6440          +7.0674\n",
      "         8       -3856.2832          +9.3607\n",
      "         9       -3842.7875         +13.4958\n",
      "        10       -3826.7496         +16.0378\n",
      "        11       -3811.1530         +15.5966\n",
      "        12       -3797.6280         +13.5250\n",
      "        13       -3786.8868         +10.7413\n",
      "        14       -3777.7363          +9.1504\n",
      "        15       -3768.1584          +9.5780\n",
      "        16       -3757.1827         +10.9757\n",
      "        17       -3745.1309         +12.0519\n",
      "        18       -3733.6290         +11.5019\n",
      "        19       -3724.5462          +9.0827\n",
      "        20       -3718.5318          +6.0145\n",
      "        21       -3715.0303          +3.5014\n",
      "        22       -3713.1320          +1.8984\n",
      "        23       -3712.0869          +1.0450\n",
      "        24       -3711.4361          +0.6508\n",
      "        25       -3710.9515          +0.4847\n",
      "        26       -3710.5356          +0.4158\n",
      "        27       -3710.1511          +0.3845\n",
      "        28       -3709.7842          +0.3669\n",
      "        29       -3709.4298          +0.3544\n",
      "        30       -3709.0859          +0.3439\n",
      "        31       -3708.7516          +0.3344\n",
      "        32       -3708.4262          +0.3254\n",
      "        33       -3708.1093          +0.3169\n",
      "        34       -3707.8007          +0.3087\n",
      "        35       -3707.4999          +0.3008\n",
      "        36       -3707.2068          +0.2931\n",
      "        37       -3706.9210          +0.2858\n",
      "        38       -3706.6424          +0.2786\n",
      "        39       -3706.3707          +0.2717\n",
      "        40       -3706.1056          +0.2650\n",
      "        41       -3705.8470          +0.2586\n",
      "        42       -3705.5946          +0.2524\n",
      "        43       -3705.3481          +0.2465\n",
      "        44       -3705.1073          +0.2408\n",
      "        45       -3704.8719          +0.2354\n",
      "        46       -3704.6416          +0.2303\n",
      "        47       -3704.4162          +0.2254\n",
      "        48       -3704.1954          +0.2209\n",
      "        49       -3703.9787          +0.2167\n",
      "        50       -3703.7659          +0.2128\n",
      "        51       -3703.5567          +0.2092\n",
      "        52       -3703.3507          +0.2060\n",
      "        53       -3703.1475          +0.2032\n",
      "        54       -3702.9468          +0.2007\n",
      "        55       -3702.7481          +0.1987\n",
      "        56       -3702.5511          +0.1970\n",
      "        57       -3702.3554          +0.1957\n",
      "        58       -3702.1606          +0.1949\n",
      "        59       -3701.9661          +0.1944\n",
      "        60       -3701.7717          +0.1944\n",
      "        61       -3701.5769          +0.1949\n",
      "        62       -3701.3811          +0.1957\n",
      "        63       -3701.1840          +0.1971\n",
      "        64       -3700.9851          +0.1989\n",
      "        65       -3700.7839          +0.2012\n",
      "        66       -3700.5799          +0.2040\n",
      "        67       -3700.3726          +0.2073\n",
      "        68       -3700.1614          +0.2111\n",
      "        69       -3699.9459          +0.2155\n",
      "        70       -3699.7255          +0.2204\n",
      "        71       -3699.4996          +0.2259\n",
      "        72       -3699.2676          +0.2320\n",
      "        73       -3699.0288          +0.2388\n",
      "        74       -3698.7826          +0.2462\n",
      "        75       -3698.5283          +0.2543\n",
      "        76       -3698.2651          +0.2632\n",
      "        77       -3697.9923          +0.2728\n",
      "        78       -3697.7090          +0.2833\n",
      "        79       -3697.4143          +0.2947\n",
      "        80       -3697.1072          +0.3071\n",
      "        81       -3696.7868          +0.3205\n",
      "        82       -3696.4518          +0.3350\n",
      "        83       -3696.1011          +0.3507\n",
      "        84       -3695.7335          +0.3676\n",
      "        85       -3695.3476          +0.3859\n",
      "        86       -3694.9421          +0.4056\n",
      "        87       -3694.5154          +0.4266\n",
      "        88       -3694.0663          +0.4491\n",
      "        89       -3693.5934          +0.4729\n",
      "        90       -3693.0957          +0.4978\n",
      "        91       -3692.5722          +0.5235\n",
      "        92       -3692.0226          +0.5495\n",
      "        93       -3691.4475          +0.5752\n",
      "        94       -3690.8482          +0.5993\n",
      "        95       -3690.2275          +0.6207\n",
      "        96       -3689.5900          +0.6375\n",
      "        97       -3688.9422          +0.6479\n",
      "        98       -3688.2925          +0.6497\n",
      "        99       -3687.6515          +0.6410\n",
      "       100       -3687.0310          +0.6205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GMMHMM(covariance_type='full',\n",
       "       covars_prior=array([[[[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]]],\n",
       "\n",
       "\n",
       "       [[[0.]],\n",
       "\n",
       "        [[0.]]]]),\n",
       "       covars_weight=array([[-3., -3.],\n",
       "       [-3., -3.],\n",
       "       [-3., -3.]]),\n",
       "       means_prior=array([[[0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [0.]]]),\n",
       "       means_weight=array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]]),\n",
       "       n_components=3, n_iter=100, n_mix=2, verbose=True,\n",
       "       weights_prior=array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hmmlearn.hmm import GMMHMM, GaussianHMM\n",
    "model = GMMHMM(3,n_mix=2, covariance_type='full', n_iter=100, verbose=True)\n",
    "model.fit(np.array(pc1).reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "816ba09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -23.076979\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.432\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "X = pc1.diff()[1:].values\n",
    "result = adfuller(X)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac6675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GMMHMM\n",
    "import numpy as np\n",
    "\n",
    "def bic_general(likelihood_fn, k, X):\n",
    "    \"\"\"likelihood_fn: Function. Should take as input X and give out   the log likelihood\n",
    "                  of the data under the fitted model.\n",
    "           k - int. Number of parameters in the model. The parameter that we are trying to optimize.\n",
    "                    For HMM it is number of states.\n",
    "                    For GMM the number of components.\n",
    "           X - array. Data that been fitted upon.\n",
    "    \"\"\"\n",
    "    bic = np.log(len(X))*k - 2*likelihood_fn(X)\n",
    "    return bic\n",
    "\n",
    "def bic_general_hist(score, k, X):\n",
    "    bic = np.log(len(X))*k - 2*score\n",
    "    return bic\n",
    "\n",
    "def bic_hmmlearn(X, min_states, max_states):\n",
    "    lowest_bic = np.infty\n",
    "    models=[]\n",
    "    bic = []\n",
    "    bic_history = []\n",
    "    n_states_range = range(min_states,max_states)\n",
    "    for n_components in n_states_range:\n",
    "        hmm_curr = GMMHMM(n_components=n_components, covariance_type='full')\n",
    "        hmm_curr.fit(X)\n",
    "        models.append(hmm_curr)\n",
    "        # Calculate number of free parameters\n",
    "        # free_parameters = for_means + for_covars + for_transmat + for_startprob\n",
    "        # for_means & for_covars = n_features*n_components\n",
    "        n_features = hmm_curr.n_features\n",
    "        n_mixtures = hmm_curr.weights_.shape[1]\n",
    "        free_parameters = 2*(n_components*n_features*n_mixtures) + n_components*(n_components-1) + (n_components-1)\n",
    "\n",
    "        # Firstly want the convergance of the BIC score to get a more detailed picture\n",
    "        history_curr = list(hmm_curr.monitor_.history)\n",
    "        bic_history_curr = map(lambda x: bic_general_hist(x, free_parameters, X), history_curr) \n",
    "        bic_history.append(bic_history_curr)\n",
    "        \n",
    "        bic_curr = bic_general(hmm_curr.score, free_parameters, X)\n",
    "        bic.append(bic_curr)\n",
    "        \n",
    "        if bic_curr < lowest_bic:\n",
    "            lowest_bic = bic_curr\n",
    "        best_hmm = hmm_curr\n",
    "    return (best_hmm, bic, bic_history, models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36459dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GMMHMM\n",
    "import numpy as np\n",
    "\n",
    "def aic_general(likelihood_fn, k, X):\n",
    "    \"\"\"likelihood_fn: Function. Should take as input X and give out   the log likelihood\n",
    "                  of the data under the fitted model.\n",
    "           k - int. Number of parameters in the model. The parameter that we are trying to optimize.\n",
    "                    For HMM it is number of states.\n",
    "                    For GMM the number of components.\n",
    "           X - array. Data that been fitted upon.\n",
    "    \"\"\"\n",
    "    aic = 2*k - 2*likelihood_fn(X)\n",
    "    return aic\n",
    "\n",
    "def aic_general_hist(score, k, X):\n",
    "    aic = 2*k - 2*score\n",
    "    return aic\n",
    "\n",
    "def aic_hmmlearn(X, min_states, max_states):\n",
    "    lowest_aic = np.infty\n",
    "    models=[]\n",
    "    aic = []\n",
    "    aic_history = []\n",
    "    n_states_range = range(min_states,max_states)\n",
    "    for n_components in n_states_range:\n",
    "        hmm_curr = GMMHMM(n_components=n_components, covariance_type='full')\n",
    "        hmm_curr.fit(X)\n",
    "        models.append(hmm_curr)\n",
    "        # Calculate number of free parameters\n",
    "        # free_parameters = for_means + for_covars + for_transmat + for_startprob\n",
    "        # for_means & for_covars = n_features*n_components\n",
    "        n_features = hmm_curr.n_features\n",
    "        n_mixtures = hmm_curr.weights_.shape[1]\n",
    "        free_parameters = 2*(n_components*n_features*n_mixtures) + n_components*(n_components-1) + (n_components-1)\n",
    "\n",
    "        # Firstly want the convergance of the BIC score to get a more detailed picture\n",
    "        history_curr = list(hmm_curr.monitor_.history)\n",
    "        aic_history_curr = map(lambda x: aic_general_hist(x, free_parameters, X), history_curr) \n",
    "        aic_history.append(aic_history_curr)\n",
    "        \n",
    "        aic_curr = aic_general(hmm_curr.score, free_parameters, X)\n",
    "        aic.append(aic_curr)\n",
    "        \n",
    "        if aic_curr < lowest_aic:\n",
    "            lowest_aic = aic_curr\n",
    "        best_hmm = hmm_curr\n",
    "    return (best_hmm, aic, aic_history, models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98bee96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = map(lambda x: bic_general(x, 20, np.array(pc1).reshape(-1, 1)),list(model.monitor_.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "53d62d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[x[0], x[1]] for x in zip(df['level'].pct_change().fillna(df['level'].pct_change().mean()), df['volatility'].fillna(df['volatility'].mean()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "abf5b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hmm, bic, bic_history, models_bic = bic_hmmlearn(X, 2, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b071f11",
   "metadata": {},
   "source": [
    "# Identifying Regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef74420",
   "metadata": {},
   "source": [
    "The problem that Faheem identified, what ramifications could it actually have? \n",
    "\n",
    "If we don't have the same number of each contract, then this could affect our final calculation as those contracts will be weighted more? \n",
    "\n",
    "This step is fine but we need to first manually identify what the best number of states to use is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf432ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's identify the regimes - best hmm says 6 components but that is far too many.  \n",
    "\n",
    "hidden_states = best_hmm.decode(np.array(pc1).reshape(-1, 1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7afd3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(df, hidden_states, start, end):\n",
    "    '''\n",
    "    Want to take a df and an array of the hidden states derived from that df,\n",
    "    and then use this to gice us a new dataframe where each column is the entries for the hs.\n",
    "    '''\n",
    "    diff = len(df) - len(hidden_states)\n",
    "    df = df[diff:]\n",
    "    try:\n",
    "        df = df.drop(columns=['hidden_states'])\n",
    "    except:\n",
    "        print('no hidden state')\n",
    "    df['hidden_states'] = hidden_states\n",
    "    result = pd.DataFrame()\n",
    "    result = result.reindex(pd.date_range(start=start, end=end, freq='B'))\n",
    "    result = result.set_index(result.index.rename(\"Date\"))\n",
    "    for hs in np.unique(hidden_states):\n",
    "        state_data = df[df['hidden_states'] == hs]\n",
    "        result = result.merge(state_data['level'].pct_change(), how='left', on='Date')\n",
    "    result.columns = np.unique(hidden_states)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "481397c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to compare the distributions at each stage, so first with two states, then with 3 states and then with 4 states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2309b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f51ca5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2981650754.py:5: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2981650754.py:5: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2981650754.py:5: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2820459684.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'], how='left', on='Date')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2981650754.py:5: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2820459684.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'], how='left', on='Date')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2981650754.py:5: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2820459684.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'], how='left', on='Date')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2820459684.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'], how='left', on='Date')\n"
     ]
    }
   ],
   "source": [
    "# Indvidual distribution plot\n",
    "#sns.kdeplot(df['volatility'].dropna(), shade=True)\n",
    "for model in models_bic:\n",
    "    hs = model.decode(X)[1]\n",
    "    hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, col in enumerate(hs_df.columns):\n",
    "        # Want a way to plot on the same axis so we can see how the distributions are broken up.  \n",
    "        state_data = hs_df[col]\n",
    "        ax.plot(state_data, label=col)\n",
    "        #sns.kdeplot(state_data, ax=ax, label=col, shade=True)\n",
    "        fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e77ab8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GMMHMM(covariance_type='full',\n",
       "       covars_prior=array([[[[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]]]]),\n",
       "       covars_weight=array([[-4.],\n",
       "       [-4.],\n",
       "       [-4.]]),\n",
       "       means_prior=array([[[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]]]),\n",
       "       means_weight=array([[0.],\n",
       "       [0.],\n",
       "       [0.]]), n_components=3,\n",
       "       weights_prior=array([[1.],\n",
       "       [1.],\n",
       "       [1.]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_bic[1].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a99270a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006323185011709587"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.returns.quantile([.25, .75])[0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4a40afb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5352437 , 0.11445363, 0.31630018, 0.03400248])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_bic[2].get_stationary_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b801fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2845496750.py:4: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden_states: 2\n",
      "Mean of hidden_state 0 = 0.00029794016037446475\n",
      "St.dev of hidden_state 0 = 0.01466214262039426\n",
      "Skewness of hidden_state 0 = 3.968596579857969\n",
      "Kurtiosis of hidden_state 0 = 92.89075287221895\n",
      "Mean of hidden_state 1 = 0.0005797149730066512\n",
      "St.dev of hidden_state 1 = 0.022324038039354496\n",
      "Skewness of hidden_state 1 = 0.4736700306954644\n",
      "Kurtiosis of hidden_state 1 = 9.57283988361247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2845496750.py:4: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden_states: 3\n",
      "Mean of hidden_state 0 = 0.0003335927252487733\n",
      "St.dev of hidden_state 0 = 0.01569599580973111\n",
      "Skewness of hidden_state 0 = 3.376488513345828\n",
      "Kurtiosis of hidden_state 0 = 68.0815620897231\n",
      "Mean of hidden_state 1 = 0.0015712211845507324\n",
      "St.dev of hidden_state 1 = 0.04038824076796987\n",
      "Skewness of hidden_state 1 = 6.519471585986223\n",
      "Kurtiosis of hidden_state 1 = 71.4544042774783\n",
      "Mean of hidden_state 2 = 0.0005836949539560401\n",
      "St.dev of hidden_state 2 = 0.023186314631595456\n",
      "Skewness of hidden_state 2 = 3.7403231828655574\n",
      "Kurtiosis of hidden_state 2 = 59.86982692633161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2845496750.py:4: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/3555715838.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'].pct_change(), how='left', on='Date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden_states: 4\n",
      "Mean of hidden_state 0 = 0.00041432627557089175\n",
      "St.dev of hidden_state 0 = 0.019152781359930236\n",
      "Skewness of hidden_state 0 = 7.484683066218673\n",
      "Kurtiosis of hidden_state 0 = 174.45368907059822\n",
      "Mean of hidden_state 1 = 0.0014833719495606912\n",
      "St.dev of hidden_state 1 = 0.03624722438199862\n",
      "Skewness of hidden_state 1 = 3.203886309659909\n",
      "Kurtiosis of hidden_state 1 = 30.161894323523022\n",
      "Mean of hidden_state 2 = 0.0004930520692104163\n",
      "St.dev of hidden_state 2 = 0.02050890713997409\n",
      "Skewness of hidden_state 2 = 2.4728131293324456\n",
      "Kurtiosis of hidden_state 2 = 36.98322873774119\n",
      "Mean of hidden_state 3 = 0.0006315973867081811\n",
      "St.dev of hidden_state 3 = 0.0339014581922025\n",
      "Skewness of hidden_state 3 = 0.30221487473809105\n",
      "Kurtiosis of hidden_state 3 = 3.348744649545466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2845496750.py:4: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/3555715838.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'].pct_change(), how='left', on='Date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden_states: 5\n",
      "Mean of hidden_state 0 = 0.00048366617169029875\n",
      "St.dev of hidden_state 0 = 0.022109721828370524\n",
      "Skewness of hidden_state 0 = 13.301310053580844\n",
      "Kurtiosis of hidden_state 0 = 359.83012005835036\n",
      "Mean of hidden_state 1 = 0.002289298472064436\n",
      "St.dev of hidden_state 1 = 0.04937520814023234\n",
      "Skewness of hidden_state 1 = 5.144253191330963\n",
      "Kurtiosis of hidden_state 1 = 45.330343848830424\n",
      "Mean of hidden_state 2 = 0.0005891005843206451\n",
      "St.dev of hidden_state 2 = 0.021552606237995965\n",
      "Skewness of hidden_state 2 = 2.042621078741421\n",
      "Kurtiosis of hidden_state 2 = 35.93901059668334\n",
      "Mean of hidden_state 3 = -0.0007507317174889755\n",
      "St.dev of hidden_state 3 = 0.03660496541147931\n",
      "Skewness of hidden_state 3 = -1.235078898998327\n",
      "Kurtiosis of hidden_state 3 = 3.700060000945416\n",
      "Mean of hidden_state 4 = 0.0012824805166466858\n",
      "St.dev of hidden_state 4 = 0.03264651792025273\n",
      "Skewness of hidden_state 4 = 3.041453912104226\n",
      "Kurtiosis of hidden_state 4 = 28.61406923179438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/2845496750.py:4: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/3555715838.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'].pct_change(), how='left', on='Date')\n",
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/3555715838.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'level_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  result = result.merge(state_data['level'].pct_change(), how='left', on='Date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden_states: 6\n",
      "Mean of hidden_state 0 = 0.0005330561483248821\n",
      "St.dev of hidden_state 0 = 0.02293248172082117\n",
      "Skewness of hidden_state 0 = 13.090760620644495\n",
      "Kurtiosis of hidden_state 0 = 338.24789591330733\n",
      "Mean of hidden_state 1 = 0.0020506247125581358\n",
      "St.dev of hidden_state 1 = 0.04700835346412734\n",
      "Skewness of hidden_state 1 = 5.177156494654461\n",
      "Kurtiosis of hidden_state 1 = 47.783946059577836\n",
      "Mean of hidden_state 2 = 0.0010998379597119876\n",
      "St.dev of hidden_state 2 = 0.03183570645232736\n",
      "Skewness of hidden_state 2 = 2.855540031135851\n",
      "Kurtiosis of hidden_state 2 = 33.494532999463665\n",
      "Mean of hidden_state 3 = 0.00067766805710924\n",
      "St.dev of hidden_state 3 = 0.03482846312507223\n",
      "Skewness of hidden_state 3 = 0.5871018194766547\n",
      "Kurtiosis of hidden_state 3 = 2.934083666947225\n",
      "Mean of hidden_state 4 = 0.0006762696417818955\n",
      "St.dev of hidden_state 4 = 0.021322651523771212\n",
      "Skewness of hidden_state 4 = 2.2695815030977355\n",
      "Kurtiosis of hidden_state 4 = 35.86846126540547\n",
      "Mean of hidden_state 5 = -0.02182211628557383\n",
      "St.dev of hidden_state 5 = 0.08257118158001803\n",
      "Skewness of hidden_state 5 = -2.695739993032395\n",
      "Kurtiosis of hidden_state 5 = 7.432363995432725\n"
     ]
    }
   ],
   "source": [
    "# Joint plot\n",
    "for model in models_bic:\n",
    "    hs_multi = model.decode(X)[1]\n",
    "    hs_df = get_state_df(df, hs_multi, '1/1/2010', '26/02/2022')\n",
    "    hs = df\n",
    "    hs['returns'] = hs['level'].pct_change()\n",
    "    hs['hidden_states'] = hs_multi\n",
    "    q1, q2 = 0.1, 0.9\n",
    "    quantiles = hs['returns'].quantile([q1, q2])\n",
    "    print(\"Number of hidden_states: {}\".format(model.n_components))\n",
    "    for col in hs_df.columns:\n",
    "        print(\"Mean of hidden_state {} = {}\".format(col, hs_df[col].dropna().mean()))\n",
    "        print(\"St.dev of hidden_state {} = {}\".format(col, hs_df[col].dropna().std()))\n",
    "        print(\"Skewness of hidden_state {} = {}\".format(col, hs_df[col].dropna().skew()))\n",
    "        print(\"Kurtiosis of hidden_state {} = {}\".format(col, hs_df[col].dropna().kurtosis()))\n",
    "    sns.jointplot(data=hs, x=\"returns\", y=\"volatility\", kind=\"kde\", hue='hidden_states', ylim=(-10, 200), xlim=quantiles, shade=True, bw_adjust=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "946c17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/nf5j5v796jjb6bncp9wnk5l80000gn/T/ipykernel_1835/3940101436.py:3: UserWarning: Parsing '26/02/2022' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'0'}>,\n",
       "        <AxesSubplot:title={'center':'1'}>],\n",
       "       [<AxesSubplot:title={'center':'2'}>, <AxesSubplot:>]], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models_bic[1]\n",
    "hs = model.decode([[x[0], x[1]] for x in zip(df['level'].fillna(df['level'].mean()), df['volatility'].fillna(df['volatility'].mean()))])[1]\n",
    "hs_df = get_state_df(df, hs, '1/1/2010', '26/02/2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9834e4e2",
   "metadata": {},
   "source": [
    "By looking at the contour plots and the training time series, we conclude that we will use 3 states which the model has identified.  These can be approximately assumed to be a state with low level, a state with high level but low vol and a state with high level and high vol.  So we have, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f21b6449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         392.5\n",
       "1                         403.0\n",
       "2                        411.25\n",
       "3                         415.0\n",
       "4                         417.5\n",
       "5                        426.25\n",
       "6                        432.25\n",
       "7                         437.0\n",
       "8                         429.5\n",
       "9                         432.5\n",
       "level                   419.675\n",
       "volatility                  NaN\n",
       "slope                      40.0\n",
       "cat_level                     3\n",
       "cat_volatility              NaN\n",
       "cat_slope         Backwardation\n",
       "hidden_states                 2\n",
       "returns               -0.065468\n",
       "Name: 2010-01-12 00:00:00, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.index[7], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "803b976d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>418.50</td>\n",
       "      <td>428.75</td>\n",
       "      <td>437.00</td>\n",
       "      <td>441.00</td>\n",
       "      <td>445.25</td>\n",
       "      <td>454.000</td>\n",
       "      <td>460.25</td>\n",
       "      <td>465.250</td>\n",
       "      <td>456.25</td>\n",
       "      <td>451.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>418.75</td>\n",
       "      <td>429.00</td>\n",
       "      <td>437.50</td>\n",
       "      <td>441.75</td>\n",
       "      <td>445.25</td>\n",
       "      <td>456.125</td>\n",
       "      <td>462.25</td>\n",
       "      <td>467.375</td>\n",
       "      <td>458.50</td>\n",
       "      <td>454.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>421.75</td>\n",
       "      <td>432.00</td>\n",
       "      <td>440.75</td>\n",
       "      <td>444.75</td>\n",
       "      <td>448.75</td>\n",
       "      <td>458.250</td>\n",
       "      <td>464.25</td>\n",
       "      <td>469.500</td>\n",
       "      <td>460.75</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>417.50</td>\n",
       "      <td>428.00</td>\n",
       "      <td>436.75</td>\n",
       "      <td>440.00</td>\n",
       "      <td>443.50</td>\n",
       "      <td>453.000</td>\n",
       "      <td>459.00</td>\n",
       "      <td>464.250</td>\n",
       "      <td>455.50</td>\n",
       "      <td>452.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-21</th>\n",
       "      <td>654.25</td>\n",
       "      <td>652.75</td>\n",
       "      <td>647.00</td>\n",
       "      <td>608.75</td>\n",
       "      <td>597.75</td>\n",
       "      <td>604.750</td>\n",
       "      <td>608.25</td>\n",
       "      <td>609.750</td>\n",
       "      <td>569.00</td>\n",
       "      <td>560.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-22</th>\n",
       "      <td>674.75</td>\n",
       "      <td>672.50</td>\n",
       "      <td>665.25</td>\n",
       "      <td>621.00</td>\n",
       "      <td>605.75</td>\n",
       "      <td>612.750</td>\n",
       "      <td>616.25</td>\n",
       "      <td>617.750</td>\n",
       "      <td>576.75</td>\n",
       "      <td>567.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-23</th>\n",
       "      <td>683.75</td>\n",
       "      <td>681.25</td>\n",
       "      <td>674.50</td>\n",
       "      <td>628.75</td>\n",
       "      <td>611.25</td>\n",
       "      <td>618.000</td>\n",
       "      <td>621.50</td>\n",
       "      <td>622.750</td>\n",
       "      <td>579.25</td>\n",
       "      <td>570.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-24</th>\n",
       "      <td>695.00</td>\n",
       "      <td>690.25</td>\n",
       "      <td>678.50</td>\n",
       "      <td>620.75</td>\n",
       "      <td>604.75</td>\n",
       "      <td>611.000</td>\n",
       "      <td>614.25</td>\n",
       "      <td>615.500</td>\n",
       "      <td>571.75</td>\n",
       "      <td>563.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-25</th>\n",
       "      <td>659.50</td>\n",
       "      <td>655.75</td>\n",
       "      <td>644.00</td>\n",
       "      <td>594.25</td>\n",
       "      <td>579.75</td>\n",
       "      <td>586.750</td>\n",
       "      <td>590.00</td>\n",
       "      <td>591.500</td>\n",
       "      <td>551.00</td>\n",
       "      <td>543.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3171 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1       2       3       4        5       6        7  \\\n",
       "Date                                                                           \n",
       "2010-01-01     NaN     NaN     NaN     NaN     NaN      NaN     NaN      NaN   \n",
       "2010-01-04  418.50  428.75  437.00  441.00  445.25  454.000  460.25  465.250   \n",
       "2010-01-05  418.75  429.00  437.50  441.75  445.25  456.125  462.25  467.375   \n",
       "2010-01-06  421.75  432.00  440.75  444.75  448.75  458.250  464.25  469.500   \n",
       "2010-01-07  417.50  428.00  436.75  440.00  443.50  453.000  459.00  464.250   \n",
       "...            ...     ...     ...     ...     ...      ...     ...      ...   \n",
       "2022-02-21  654.25  652.75  647.00  608.75  597.75  604.750  608.25  609.750   \n",
       "2022-02-22  674.75  672.50  665.25  621.00  605.75  612.750  616.25  617.750   \n",
       "2022-02-23  683.75  681.25  674.50  628.75  611.25  618.000  621.50  622.750   \n",
       "2022-02-24  695.00  690.25  678.50  620.75  604.75  611.000  614.25  615.500   \n",
       "2022-02-25  659.50  655.75  644.00  594.25  579.75  586.750  590.00  591.500   \n",
       "\n",
       "                 8       9  \n",
       "Date                        \n",
       "2010-01-01     NaN     NaN  \n",
       "2010-01-04  456.25  451.25  \n",
       "2010-01-05  458.50  454.50  \n",
       "2010-01-06  460.75  457.75  \n",
       "2010-01-07  455.50  452.50  \n",
       "...            ...     ...  \n",
       "2022-02-21  569.00  560.25  \n",
       "2022-02-22  576.75  567.75  \n",
       "2022-02-23  579.25  570.00  \n",
       "2022-02-24  571.75  563.50  \n",
       "2022-02-25  551.00  543.00  \n",
       "\n",
       "[3171 rows x 10 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b841bb0",
   "metadata": {},
   "source": [
    "# KMO and Barlett Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a4606077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Sep 10 20:21:04 2016\n",
    "Bartlett Sphericity Test Function:\n",
    "- This test evaluates sampling adequacy for exploratory Factor Analysis\n",
    "Bartlett_Sphericity function has two inputs:\n",
    "- The Dataset (numerical or ordinal variables only)\n",
    "- The correlation method (spearman or pearson)\n",
    "It Outputs the test result, degrees of freedom and p-value\n",
    "@authors: Rui Sarmento\n",
    "          Vera Costa\n",
    "\"\"\"\n",
    "\n",
    "#Bartlett Sphericity Test\n",
    "#Exploratory factor analysis is only useful if the matrix of population \n",
    "#correlation is statistically different from the identity matrix. \n",
    "#If these are equal, the variables are few interrelated, i.e., the specific \n",
    "#factors explain the greater proportion of the variance and the common factors\n",
    "#are unimportant. Therefore, it should be defined when the correlations \n",
    "#between the original variables are sufficiently high. \n",
    "#Thus, the factor analysis is useful in estimation of common factors. \n",
    "#With this in mind, the Bartlett Sphericity test can be used. The hypotheses are:\n",
    "\n",
    "# H0: the matrix of population correlations is equal to the identity matrix\n",
    "# H1: the matrix of population correlations is different from the identity matrix.\n",
    "\n",
    "def bartlett_sphericity(dataset, corr_method=\"pearson\"):\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : dataframe, mandatory (numerical or ordinal variables)\n",
    "        \n",
    "    corr_method : {'pearson', 'spearman'}, optional\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : namedtuple\n",
    "        The function outputs the test value (chi2), the degrees of freedom (ddl)\n",
    "        and the p-value.\n",
    "        It also delivers the n_p_ratio if the number of instances (n) divided \n",
    "        by the numbers of variables (p) is more than 5. A warning might be issued.\n",
    "        \n",
    "        Ex:\n",
    "        chi2:  410.27280642443156\n",
    "        ddl:  45.0\n",
    "        p-value:  8.73359410503e-61\n",
    "        n_p_ratio:    20.00\n",
    "        \n",
    "        Out: Bartlett_Sphericity_Test_Results(chi2=410.27280642443156, ddl=45.0, pvalue=8.7335941050291506e-61)\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Bartlett,  M.  S.,  (1951),  The  Effect  of  Standardization  on  a  chi  square  Approximation  in  Factor\n",
    "    Analysis, Biometrika, 38, 337-344.\n",
    "    [2] R. Sarmento and V. Costa, (2017)\n",
    "    \"Comparative Approaches to Using R and Python for Statistical Data Analysis\", IGI-Global.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    illustration how to use the function.\n",
    "    \n",
    "    >>> bartlett_sphericity(survey_data, corr_method=\"spearman\")\n",
    "    chi2:  410.27280642443145\n",
    "    ddl:  45.0\n",
    "    p-value:  8.73359410503e-61\n",
    "    n_p_ratio:    20.00\n",
    "    C:\\Users\\Rui Sarmento\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\start_ipython_kernel.py:75: \n",
    "    UserWarning: NOTE: we advise  to  use  this  test  only  if  the number of instances (n) divided by the number of variables (p) is lower than 5. Please try the KMO test, for example.\n",
    "    backend_o = CONF.get('ipython_console', 'pylab/backend', 0)\n",
    "    Out[12]: Bartlett_Sphericity_Test_Results(chi2=410.27280642443156, ddl=45.0, pvalue=8.7335941050291506e-61)\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import math as math\n",
    "    import scipy.stats as stats\n",
    "    import warnings as warnings\n",
    "    import collections\n",
    "\n",
    "    #Dimensions of the Dataset\n",
    "    n = dataset.shape[0]\n",
    "    p = dataset.shape[1]\n",
    "    n_p_ratio = n / p\n",
    "    \n",
    "    #Several Calculations\n",
    "    chi2 = - (n - 1 - (2 * p + 5) / 6) * math.log(np.linalg.det(dataset.corr(method=corr_method)))\n",
    "    #Freedom Degree\n",
    "    ddl = p * (p - 1) / 2\n",
    "    #p-value\n",
    "    pvalue = 1 - stats.chi2.cdf(chi2 , ddl)\n",
    "    \n",
    "    Result = collections.namedtuple(\"Bartlett_Sphericity_Test_Results\", [\"chi2\", \"ddl\", \"pvalue\"], verbose=False, rename=False)   \n",
    "    \n",
    "    #Output of the results - named tuple\n",
    "    result = Result(chi2=chi2,ddl=ddl,pvalue=pvalue) \n",
    "\n",
    "    \n",
    "    #Output of the function\n",
    "    if n_p_ratio > 5 :\n",
    "        print(\"n_p_ratio: {0:8.2f}\".format(n_p_ratio))\n",
    "        warnings.warn(\"NOTE: we advise  to  use  this  test  only  if  the number of instances (n) divided by the number of variables (p) is lower than 5. Please try the KMO test, for example.\")\n",
    "        \n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Created on Sat Sep 10 20:21:04 2016\n",
    "KMO Test Function:\n",
    "- This test evaluates sampling adequacy for exploratory Factor Analysis\n",
    "KMO Test function has one input:\n",
    "- The Dataset Correlation Matrix\n",
    "It Outputs the test result, and the results per variable\n",
    "@authors: Rui Sarmento\n",
    "          Vera Costa\n",
    "\"\"\"\n",
    "\n",
    "#KMO Test\n",
    "#KMO is a measure of the adequacy of sampling “Kaiser-Meyer-Olkin\" and checks \n",
    "#if it is possible to factorize the main variables efficiently.\n",
    "#The correlation matrix is always the starting point. The variables are more or\n",
    "#less correlated, but the others can influence the correlation between the two \n",
    "#variables. Hence, with KMO, the partial correlation is used to measure the \n",
    "#relation between two variables by removing the effect of the remaining variables.\n",
    "\n",
    "def kmo(dataset_corr):\n",
    "    \n",
    "    import numpy as np\n",
    "    import math as math\n",
    "    import collections\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_corr : ndarray\n",
    "        Array containing dataset correlation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : namedtuple\n",
    "        The function outputs the test value (value), the test value per variable (per_variable)\n",
    "       \n",
    "        Ex:\n",
    "        Out[30]: \n",
    "        KMO_Test_Results(value=0.798844102413, \n",
    "        per_variable=\n",
    "        Q1     0.812160468405\n",
    "        Q2     0.774161264483\n",
    "        Q3     0.786819432663\n",
    "        Q4     0.766251123086\n",
    "        Q5     0.800579196084\n",
    "        Q6     0.842927745203 \n",
    "        Q7     0.792010173432 \n",
    "        Q8     0.862037322891\n",
    "        Q9     0.714795031915 \n",
    "        Q10    0.856497242574\n",
    "        dtype: float64)\n",
    "    \n",
    "    References\n",
    "    ----------    \n",
    "    [1] Kaiser, H. F. (1970). A second generation little jiffy. Psychometrika, 35(4), 401-415.\n",
    "    [2] Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39(1), 31-36.\n",
    "    [3] R. Sarmento and V. Costa, (2017)\n",
    "    \"Comparative Approaches to Using R and Python for Statistical Data Analysis\", IGI-Global\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    illustration how to use the function.\n",
    "    \n",
    "    >>> kmo_test(survey_data.corr(method=\"spearman\"))\n",
    "         \n",
    "        KMO_Test_Results(value=0.798844102413, \n",
    "        per_variable=\n",
    "        Q1     0.812160468405\n",
    "        Q2     0.774161264483\n",
    "        Q3     0.786819432663\n",
    "        Q4     0.766251123086\n",
    "        Q5     0.800579196084\n",
    "        Q6     0.842927745203 \n",
    "        Q7     0.792010173432 \n",
    "        Q8     0.862037322891\n",
    "        Q9     0.714795031915 \n",
    "        Q10    0.856497242574\n",
    "        dtype: float64) \n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    #KMO Test\n",
    "    #inverse of the correlation matrix\n",
    "    corr_inv = np.linalg.inv(dataset_corr)\n",
    "    nrow_inv_corr, ncol_inv_corr = dataset_corr.shape\n",
    "    \n",
    "    #partial correlation matrix\n",
    "    A = np.ones((nrow_inv_corr,ncol_inv_corr))\n",
    "    for i in range(0,nrow_inv_corr,1):\n",
    "        for j in range(i,ncol_inv_corr,1):\n",
    "            #above the diagonal\n",
    "            A[i,j] = - (corr_inv[i,j]) / (math.sqrt(corr_inv[i,i] * corr_inv[j,j]))\n",
    "            #below the diagonal\n",
    "            A[j,i] = A[i,j]\n",
    "    \n",
    "    #transform to an array of arrays (\"matrix\" with Python)\n",
    "    dataset_corr = np.asarray(dataset_corr)\n",
    "        \n",
    "    #KMO value\n",
    "    kmo_num = np.sum(np.square(dataset_corr)) - np.sum(np.square(np.diagonal(dataset_corr)))\n",
    "    kmo_denom = kmo_num + np.sum(np.square(A)) - np.sum(np.square(np.diagonal(A)))\n",
    "    kmo_value = kmo_num / kmo_denom\n",
    "    \n",
    "    \n",
    "    kmo_j = [None]*dataset_corr.shape[1]\n",
    "    #KMO per variable (diagonal of the spss anti-image matrix)\n",
    "    for j in range(0, dataset_corr.shape[1]):\n",
    "        kmo_j_num = np.sum(dataset_corr[:,[j]] ** 2) - dataset_corr[j,j] ** 2\n",
    "        kmo_j_denom = kmo_j_num + np.sum(A[:,[j]] ** 2) - A[j,j] ** 2\n",
    "        kmo_j[j] = kmo_j_num / kmo_j_denom\n",
    "\n",
    "    \n",
    "    Result = collections.namedtuple(\"KMO_Test_Results\", [\"value\", \"per_variable\"])   \n",
    "    \n",
    "    #Output of the results - named tuple    \n",
    "    return Result(value=kmo_value,per_variable=kmo_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27d5ab",
   "metadata": {},
   "source": [
    "# Building Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "302cb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_HMM_Strategy():\n",
    "    def __init__(self, holding_period:int, test_size:float, df, n_components:int, pca_thresh: float):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        self.holding_period = holding_period\n",
    "        self.test_size = test_size\n",
    "        self.df = df\n",
    "        self.train_df, self.test_df = train_test_split(df, test_size=test_size, shuffle=False)\n",
    "        self.n_components = n_components\n",
    "        self.pca_thresh = pca_thresh\n",
    "        \n",
    "    def __identify_regimes__(self, X, covariance_type):\n",
    "        import hmmlearn as hmm\n",
    "        model = GMMHMM(n_components=self.n_components, covariance_type=covariance_type)\n",
    "        model.fit(X)\n",
    "        self.train_df['hidden_states'] = model.decode(X)[1]\n",
    "        \n",
    "    def __get_state_df__(self, component):\n",
    "        return self.train_df[self.train_df.hidden_states == component]\n",
    "\n",
    "    def __pca__(self, X, n_components):\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        import numpy as np\n",
    "        print(kmo(X.dropna().corr()))\n",
    "        pca = PCA(n_components)\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        print(X_scaled)\n",
    "        pca.fit(X_scaled)\n",
    "        scores = pca.transform(X_scaled)\n",
    "        loadings = pca.components_\n",
    "        reconstructed_data = np.dot(scores, loadings)\n",
    "        eig_vals = pca.explained_variance_\n",
    "        return scores, loadings, reconstructed_data, eig_vals\n",
    "\n",
    "    def __regime_pca__(self, n_components):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        pcs = dict()\n",
    "        for hs in range(n_components):\n",
    "            hs_df = self.__get_state_df__(hs).iloc[:, :-1]\n",
    "            hs_df = hs_df.fillna(hs_df.mean())\n",
    "            scores, loadings, reconstructed_data, eig_vals = self.__pca__(hs_df, n_components)\n",
    "            residuals = StandardScaler().fit_transform(hs_df) - reconstructed_data\n",
    "            pcs[hs] = {'scores': scores,\n",
    "                      'loadings': loadings,\n",
    "                       'reconstructed_data': reconstructed_data,\n",
    "                       'eig_vals': eig_vals,\n",
    "                       'residuals': residuals\n",
    "                      }\n",
    "        self.pcs = pcs\n",
    "            \n",
    "    def __get_residuals__(self, date_idx):\n",
    "        return self.residuals[date_ix]\n",
    "    \n",
    "    def __get_curr_regime__(self, date_idx):\n",
    "        return df.loc[date_idx, 'hidden_states']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3a925d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = PCA_HMM_Strategy(30, 0.4, df.iloc[:, :10], 3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "4e90a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to define some derived variables from the data that we have to feed into our HMM.\n",
    "\n",
    "# LEVEL \n",
    "level = strategy.train_df.mean(axis=1).dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "ef991817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility \n",
    "\n",
    "# We want the moving average over an x-day window\n",
    "\n",
    "def mse_volatility(data):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    mean = data.mean()\n",
    "    means = np.full((len(data)), mean)\n",
    "    return mse(data, means)\n",
    "\n",
    "volatility = level.rolling(10, min_periods=10).apply(mse_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "81e45bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_df = strategy.train_df.copy()\n",
    "strat_df['level'] = level\n",
    "strat_df['volatility'] = volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "947360cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[x[0], x[1]] for x in zip(strat_df['level'].pct_change().fillna(strat_df['level'].pct_change().mean()), strat_df['volatility'].fillna(strat_df['volatility'].mean()))]\n",
    "strategy.__identify_regimes__(X, 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "bfd92794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMO_Test_Results(value=0.8179358263827342, per_variable=[0.911658783501279, 0.8363960473705255, 0.8397760413109135, 0.7546376309799757, 0.7756001952313565, 0.8930040583494051, 0.8425613959445252, 0.8252906973583349, 0.7591468607807325, 0.7732477525422278])\n",
      "[[ 0.00000000e+00  0.00000000e+00  8.44131976e-16 ...  1.28202582e-15\n",
      "  -1.41287586e-15 -1.51376880e-15]\n",
      " [-1.16923490e+00 -1.08332276e+00 -1.02623610e+00 ... -8.89350746e-01\n",
      "  -1.01974375e+00 -1.08530157e+00]\n",
      " [-1.16748489e+00 -1.08153574e+00 -1.02252356e+00 ... -8.65387507e-01\n",
      "  -9.91781224e-01 -1.04202701e+00]\n",
      " ...\n",
      " [-1.57873651e+00 -1.52471692e+00 -1.49401509e+00 ... -1.56032144e+00\n",
      "  -1.70948595e+00 -1.74107769e+00]\n",
      " [-1.59098656e+00 -1.53722607e+00 -1.50515268e+00 ... -1.57723667e+00\n",
      "  -1.72502068e+00 -1.75772176e+00]\n",
      " [-1.59798659e+00 -1.54616117e+00 -1.51629028e+00 ... -1.58851349e+00\n",
      "  -1.72502068e+00 -1.75106413e+00]]\n",
      "KMO_Test_Results(value=0.7999684845382311, per_variable=[0.8399982425511915, 0.7812453954722024, 0.7663583640331916, 0.7713559867508658, 0.7872626475117385, 0.8758121395137447, 0.8028330850256451, 0.8142608947704523, 0.7739372881021401, 0.7990298110333801])\n",
      "[[-0.60249787 -0.57799765 -0.56021216 ... -0.5048419  -0.57002516\n",
      "  -0.61491087]\n",
      " [-0.63529709 -0.61213314 -0.59924094 ... -0.55933178 -0.63454344\n",
      "  -0.66841216]\n",
      " [-0.60659778 -0.58226458 -0.56480378 ... -0.51846437 -0.56644081\n",
      "  -0.61873239]\n",
      " ...\n",
      " [-0.53074958 -0.53746175 -0.54414149 ... -0.64787784 -0.66321822\n",
      "  -0.59580327]\n",
      " [-0.54919914 -0.5577297  -0.56480378 ... -0.6802312  -0.69368519\n",
      "  -0.62637543]\n",
      " [-0.5676487  -0.57799765 -0.58546608 ... -0.71258457 -0.72415215\n",
      "  -0.6569476 ]]\n",
      "KMO_Test_Results(value=0.7919165927055528, per_variable=[0.8979216278083832, 0.7727095107854256, 0.7737308591800757, 0.7476685499942208, 0.7981202833097985, 0.8568070297840124, 0.7901030805549382, 0.776342688024013, 0.7506302843196119, 0.7826715361065618])\n",
      "[[-2.45204104 -2.37092294 -2.35001993 ... -2.22572732 -2.26901545\n",
      "  -2.22907648]\n",
      " [-2.46270604 -2.38209196 -2.3606885  ... -2.24390053 -2.28129579\n",
      "  -2.24157451]\n",
      " [-2.47337105 -2.39326098 -2.37135706 ... -2.26207373 -2.29357612\n",
      "  -2.25407254]\n",
      " ...\n",
      " [-2.44019103 -2.41808103 -2.44070274 ... -2.91227065 -2.77660264\n",
      "  -2.66650754]\n",
      " [-2.5729111  -2.53969928 -2.56605838 ... -2.87592424 -2.7274813\n",
      "  -2.61234941]\n",
      " [-2.62742112 -2.59430339 -2.61139979 ... -2.88803971 -2.73157474\n",
      "  -2.61651542]]\n"
     ]
    }
   ],
   "source": [
    "strategy.__regime_pca__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "78eb5e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                 0        1       2        3        4        5        6  \\\n",
       " Date                                                                      \n",
       " 2010-01-01     NaN      NaN     NaN      NaN      NaN      NaN      NaN   \n",
       " 2010-01-04  418.50  428.750  437.00  441.000  445.250  454.000  460.250   \n",
       " 2010-01-05  418.75  429.000  437.50  441.750  445.250  456.125  462.250   \n",
       " 2010-01-06  421.75  432.000  440.75  444.750  448.750  458.250  464.250   \n",
       " 2010-01-07  417.50  428.000  436.75  440.000  443.500  453.000  459.000   \n",
       " ...            ...      ...     ...      ...      ...      ...      ...   \n",
       " 2017-04-11  366.50  373.750  380.75  390.500  399.250  405.000  410.000   \n",
       " 2017-04-12  369.00  376.000  382.75  392.500  401.000  406.750  411.500   \n",
       " 2017-04-13  371.00  378.000  384.50  394.500  403.500  409.250  413.750   \n",
       " 2017-04-14  368.75  375.625  382.25  392.375  401.625  407.250  411.625   \n",
       " 2017-04-17  366.50  373.250  380.00  390.250  399.750  405.250  409.500   \n",
       " \n",
       "                   7        8       9    level  \n",
       " Date                                           \n",
       " 2010-01-01      NaN      NaN     NaN      NaN  \n",
       " 2010-01-04  465.250  456.250  451.25  445.750  \n",
       " 2010-01-05  467.375  458.500  454.50  447.100  \n",
       " 2010-01-06  469.500  460.750  457.75  449.850  \n",
       " 2010-01-07  464.250  455.500  452.50  445.000  \n",
       " ...             ...      ...     ...      ...  \n",
       " 2017-04-11  404.250  406.250  413.00  394.925  \n",
       " 2017-04-12  405.750  407.000  414.00  396.625  \n",
       " 2017-04-13  407.250  407.750  414.50  398.400  \n",
       " 2017-04-14  404.875  405.625  412.50  396.250  \n",
       " 2017-04-17  402.500  403.500  410.50  394.100  \n",
       " \n",
       " [1902 rows x 11 columns],\n",
       "                  0       1       2       3       4       5       6       7  \\\n",
       " Date                                                                         \n",
       " 2017-04-18  361.75  368.25  375.75  386.25  396.00  402.00  406.75  400.25   \n",
       " 2017-04-19  361.75  368.25  375.75  386.50  396.25  401.75  406.50  400.00   \n",
       " 2017-04-20  357.75  364.25  371.75  382.50  392.25  398.25  403.00  396.25   \n",
       " 2017-04-21  357.00  363.75  371.00  382.00  391.75  397.75  402.25  396.50   \n",
       " 2017-04-24  359.25  365.50  372.50  383.50  393.25  399.75  404.25  397.75   \n",
       " ...            ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 2022-02-21  654.25  652.75  647.00  608.75  597.75  604.75  608.25  609.75   \n",
       " 2022-02-22  674.75  672.50  665.25  621.00  605.75  612.75  616.25  617.75   \n",
       " 2022-02-23  683.75  681.25  674.50  628.75  611.25  618.00  621.50  622.75   \n",
       " 2022-02-24  695.00  690.25  678.50  620.75  604.75  611.00  614.25  615.50   \n",
       " 2022-02-25  659.50  655.75  644.00  594.25  579.75  586.75  590.00  591.50   \n",
       " \n",
       "                  8       9    level  \n",
       " Date                                 \n",
       " 2017-04-18  401.25  408.25  390.650  \n",
       " 2017-04-19  401.00  408.00  390.575  \n",
       " 2017-04-20  397.25  404.25  386.750  \n",
       " 2017-04-21  397.50  404.25  386.375  \n",
       " 2017-04-24  399.00  405.50  388.025  \n",
       " ...            ...     ...      ...  \n",
       " 2022-02-21  569.00  560.25  611.250  \n",
       " 2022-02-22  576.75  567.75  623.050  \n",
       " 2022-02-23  579.25  570.00  629.100  \n",
       " 2022-02-24  571.75  563.50  626.525  \n",
       " 2022-02-25  551.00  543.00  599.550  \n",
       " \n",
       " [1269 rows x 11 columns]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split(strat_df, test_size=0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9e08b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.570475</td>\n",
       "      <td>-0.575435</td>\n",
       "      <td>-0.580684</td>\n",
       "      <td>-0.582445</td>\n",
       "      <td>-0.579382</td>\n",
       "      <td>-0.561560</td>\n",
       "      <td>-0.550707</td>\n",
       "      <td>-0.552971</td>\n",
       "      <td>-0.554506</td>\n",
       "      <td>-0.555765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.602835</td>\n",
       "      <td>-0.608787</td>\n",
       "      <td>-0.617272</td>\n",
       "      <td>-0.626813</td>\n",
       "      <td>-0.625535</td>\n",
       "      <td>-0.611650</td>\n",
       "      <td>-0.602624</td>\n",
       "      <td>-0.606455</td>\n",
       "      <td>-0.611070</td>\n",
       "      <td>-0.611171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.576753</td>\n",
       "      <td>-0.581542</td>\n",
       "      <td>-0.586303</td>\n",
       "      <td>-0.587518</td>\n",
       "      <td>-0.584548</td>\n",
       "      <td>-0.568278</td>\n",
       "      <td>-0.558031</td>\n",
       "      <td>-0.560043</td>\n",
       "      <td>-0.561163</td>\n",
       "      <td>-0.562369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.617587</td>\n",
       "      <td>-0.622979</td>\n",
       "      <td>-0.629453</td>\n",
       "      <td>-0.634982</td>\n",
       "      <td>-0.633081</td>\n",
       "      <td>-0.619928</td>\n",
       "      <td>-0.610964</td>\n",
       "      <td>-0.613786</td>\n",
       "      <td>-0.616478</td>\n",
       "      <td>-0.616978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.595905</td>\n",
       "      <td>-0.601057</td>\n",
       "      <td>-0.605079</td>\n",
       "      <td>-0.600128</td>\n",
       "      <td>-0.594255</td>\n",
       "      <td>-0.563976</td>\n",
       "      <td>-0.547352</td>\n",
       "      <td>-0.549032</td>\n",
       "      <td>-0.548720</td>\n",
       "      <td>-0.551666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.561671</td>\n",
       "      <td>-0.561395</td>\n",
       "      <td>-0.557145</td>\n",
       "      <td>-0.560752</td>\n",
       "      <td>-0.566597</td>\n",
       "      <td>-0.615223</td>\n",
       "      <td>-0.633004</td>\n",
       "      <td>-0.630346</td>\n",
       "      <td>-0.626081</td>\n",
       "      <td>-0.622172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-0.540835</td>\n",
       "      <td>-0.540607</td>\n",
       "      <td>-0.536889</td>\n",
       "      <td>-0.541822</td>\n",
       "      <td>-0.547976</td>\n",
       "      <td>-0.596923</td>\n",
       "      <td>-0.615017</td>\n",
       "      <td>-0.612636</td>\n",
       "      <td>-0.608979</td>\n",
       "      <td>-0.604894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.521315</td>\n",
       "      <td>-0.520920</td>\n",
       "      <td>-0.516976</td>\n",
       "      <td>-0.521599</td>\n",
       "      <td>-0.527753</td>\n",
       "      <td>-0.576833</td>\n",
       "      <td>-0.595084</td>\n",
       "      <td>-0.592604</td>\n",
       "      <td>-0.588828</td>\n",
       "      <td>-0.584763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.540581</td>\n",
       "      <td>-0.540226</td>\n",
       "      <td>-0.536664</td>\n",
       "      <td>-0.543504</td>\n",
       "      <td>-0.550619</td>\n",
       "      <td>-0.604469</td>\n",
       "      <td>-0.624748</td>\n",
       "      <td>-0.622427</td>\n",
       "      <td>-0.619146</td>\n",
       "      <td>-0.614481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>-0.559846</td>\n",
       "      <td>-0.559531</td>\n",
       "      <td>-0.556351</td>\n",
       "      <td>-0.565409</td>\n",
       "      <td>-0.573485</td>\n",
       "      <td>-0.632104</td>\n",
       "      <td>-0.654412</td>\n",
       "      <td>-0.652251</td>\n",
       "      <td>-0.649463</td>\n",
       "      <td>-0.644198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1166 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.570475 -0.575435 -0.580684 -0.582445 -0.579382 -0.561560 -0.550707   \n",
       "1    -0.602835 -0.608787 -0.617272 -0.626813 -0.625535 -0.611650 -0.602624   \n",
       "2    -0.576753 -0.581542 -0.586303 -0.587518 -0.584548 -0.568278 -0.558031   \n",
       "3    -0.617587 -0.622979 -0.629453 -0.634982 -0.633081 -0.619928 -0.610964   \n",
       "4    -0.595905 -0.601057 -0.605079 -0.600128 -0.594255 -0.563976 -0.547352   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1161 -0.561671 -0.561395 -0.557145 -0.560752 -0.566597 -0.615223 -0.633004   \n",
       "1162 -0.540835 -0.540607 -0.536889 -0.541822 -0.547976 -0.596923 -0.615017   \n",
       "1163 -0.521315 -0.520920 -0.516976 -0.521599 -0.527753 -0.576833 -0.595084   \n",
       "1164 -0.540581 -0.540226 -0.536664 -0.543504 -0.550619 -0.604469 -0.624748   \n",
       "1165 -0.559846 -0.559531 -0.556351 -0.565409 -0.573485 -0.632104 -0.654412   \n",
       "\n",
       "             7         8         9  \n",
       "0    -0.552971 -0.554506 -0.555765  \n",
       "1    -0.606455 -0.611070 -0.611171  \n",
       "2    -0.560043 -0.561163 -0.562369  \n",
       "3    -0.613786 -0.616478 -0.616978  \n",
       "4    -0.549032 -0.548720 -0.551666  \n",
       "...        ...       ...       ...  \n",
       "1161 -0.630346 -0.626081 -0.622172  \n",
       "1162 -0.612636 -0.608979 -0.604894  \n",
       "1163 -0.592604 -0.588828 -0.584763  \n",
       "1164 -0.622427 -0.619146 -0.614481  \n",
       "1165 -0.652251 -0.649463 -0.644198  \n",
       "\n",
       "[1166 rows x 10 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(strategy.pcs[0]['reconstructed_data']).set_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5871f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
